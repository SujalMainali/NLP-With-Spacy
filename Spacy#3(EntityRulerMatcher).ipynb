{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "text=\"West Chesterre was references in Mr. Deeds.\"\n",
    "doc=nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are two differerent approaches to NLP with spacy.\n",
    "#1.Rules based approach :- This refers to providing rules to process words in textv\n",
    "#2.Machine learning approach:- This refers to usiing machine learning to process words in text\n",
    "\n",
    "#We need to use one of these approaches to prepare the model to work on text for a certain domain in which the off the shelf model fails.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "West Chesterre NORP\n",
      "Deeds PERSON\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'summary': {'tok2vec': {'assigns': ['doc.tensor'],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'tagger': {'assigns': ['token.tag'],\n",
       "   'requires': [],\n",
       "   'scores': ['tag_acc'],\n",
       "   'retokenizes': False},\n",
       "  'parser': {'assigns': ['token.dep',\n",
       "    'token.head',\n",
       "    'token.is_sent_start',\n",
       "    'doc.sents'],\n",
       "   'requires': [],\n",
       "   'scores': ['dep_uas',\n",
       "    'dep_las',\n",
       "    'dep_las_per_type',\n",
       "    'sents_p',\n",
       "    'sents_r',\n",
       "    'sents_f'],\n",
       "   'retokenizes': False},\n",
       "  'attribute_ruler': {'assigns': [],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'lemmatizer': {'assigns': ['token.lemma'],\n",
       "   'requires': [],\n",
       "   'scores': ['lemma_acc'],\n",
       "   'retokenizes': False},\n",
       "  'ner': {'assigns': ['doc.ents', 'token.ent_iob', 'token.ent_type'],\n",
       "   'requires': [],\n",
       "   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n",
       "   'retokenizes': False},\n",
       "  'entity_ruler': {'assigns': ['doc.ents', 'token.ent_type', 'token.ent_iob'],\n",
       "   'requires': [],\n",
       "   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n",
       "   'retokenizes': False}},\n",
       " 'problems': {'tok2vec': [],\n",
       "  'tagger': [],\n",
       "  'parser': [],\n",
       "  'attribute_ruler': [],\n",
       "  'lemmatizer': [],\n",
       "  'ner': [],\n",
       "  'entity_ruler': []},\n",
       " 'attrs': {'token.is_sent_start': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.head': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.ent_iob': {'assigns': ['ner', 'entity_ruler'], 'requires': []},\n",
       "  'token.lemma': {'assigns': ['lemmatizer'], 'requires': []},\n",
       "  'token.dep': {'assigns': ['parser'], 'requires': []},\n",
       "  'doc.ents': {'assigns': ['ner', 'entity_ruler'], 'requires': []},\n",
       "  'doc.sents': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.ent_type': {'assigns': ['ner', 'entity_ruler'], 'requires': []},\n",
       "  'doc.tensor': {'assigns': ['tok2vec'], 'requires': []},\n",
       "  'token.tag': {'assigns': ['tagger'], 'requires': []}}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Rules Based Approach\n",
    "#EntityRuler is a grammetical rules thay can be used to assign entity labels to tokens\n",
    "\n",
    "#The entity ruler is a list of disctionary which specifies which label to assign to a apecific psttern i.e token.\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "\n",
    "patterns=[\n",
    "    {\"label\":\"GRE\",\"pattern\":\"West Chesterre\"},\n",
    "]\n",
    "ruler.add_patterns(patterns)\n",
    "doc=nlp(text)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text,ent.label_)\n",
    "\n",
    "#This will still print West Chesterre as label other than GRE.\n",
    "#It i because the pipeline is sequential since the entity ruler was added at last it will work on the data after the NER so the labe is assigned before entity ruler can act o it\n",
    "\n",
    "nlp.analyze_pipes()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple TCOM\n",
      "U.K. GPE\n",
      "$1 billion MONEY\n",
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'entity_ruler', 'ner']\n"
     ]
    }
   ],
   "source": [
    "#To make sure that entity ruler act on data before the NER we should apply it to pipeline before NER\n",
    "nlp1 = spacy.load('en_core_web_sm')\n",
    "\n",
    "text = \"Apple is looking at buying U.K. startup for $1 billion\"\n",
    "\n",
    "ruler1=nlp1.add_pipe(\"entity_ruler\",before='ner')\n",
    "\n",
    "patterns=[\n",
    "    {'label':'TCOM','pattern':'Apple'}\n",
    "]\n",
    "ruler1.add_patterns(patterns)\n",
    "\n",
    "doc = nlp1(text)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n",
    "\n",
    "print(nlp1.pipe_names)\n",
    "\n",
    "#With the hep of list of disctionary we can add multiple patterns to make the model custom to our specific dataset.\n",
    "\n",
    "#This entity ruler is one of the ways to solve Toponym resolution problem i.e. the same named entity that can have different labels depending upon the context.capitalize\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(16571425990740197027, 6, 8)]\n",
      "EMAIL_ADDRESS\n",
      "abc@gmail.com Apple\n"
     ]
    }
   ],
   "source": [
    "#The matcher is used to apply pattern in data which is not necessary to add as a entity but as a certain structure in data.\n",
    "#We use matcher to implement a pattern which is not necessarily dependent upon the pattern of speech.\n",
    "\n",
    "from spacy.matcher import Matcher\n",
    "nlp2= spacy.load('en_core_web_sm')\n",
    "matcher= Matcher(nlp2.vocab)\n",
    "pattern=[{\"LIKE_EMAIL\":True},{\"POS\":\"PROPN\"}] #This is one of the matcher in the spacy. It takes the content of the text to find the Email like text\n",
    "matcher.add(\"EMAIL_ADDRESS\",[pattern])\n",
    "\n",
    "doc2=nlp2(\"I like to send email to abc@gmail.com Apple\")\n",
    "\n",
    "matches=matcher(doc2)\n",
    "print(matches) #This prints a list of tuple in which first item corresponds to the matched pattern name and the furthur 2 gives the location of the matched token\n",
    "\n",
    "print(nlp2.vocab[matches[0][0]].text)#We can extract which pattern is matched like this\n",
    "print(doc2[matches[0][1]:matches[0][2]])#We can get the matched token like this\n",
    "\n",
    "#There are many parameters that we can add to the matcher to make it more customized for our domain of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Martin Luther King Jr. (born Michael King Jr.; January 15, 1929 â€“ April 4, 1968) was an American Baptist minister and activist who became the most visible spokesman and leader in the American civil rights movement from 1955 until his assassination in 1968. King advanced civil rights through nonviolence and civil disobedience, inspired by his Christian beliefs and the nonviolent activism of Mahatma Gandhi. He was the son of early civil rights activist and minister Martin Luther King Sr.\n",
      "\n",
      "King participated in and led marches for blacks' right to vote, desegregation, labor rights, and other basic civil rights.[1] King led the 1955 Montgomery bus boycott and later became the first president of the Southern Christian Leadership Conference (SCLC). As president of the SCLC, he led the unsuccessful Albany Movement in Albany, Georgia, and helped organize some of the nonviolent 1963 protests in Birmingham, Alabama. King helped organize the 1963 March on Washington, where he delivered his famous \"I Have a Dream\" speech on the steps of the Lincoln Memorial.\n",
      "\n",
      "The SCLC put into practice the tactics of nonviolent protest with some success by strategically choosing the methods and places in which protests were carried out. There were several dramatic stand-offs with segregationist authorities, who sometimes turned violent.[2] Federal Bureau of Investigation (FBI) Director J. Edgar Hoover considered King a radical and made him an object of the FBI's COINTELPRO from 1963, forward. FBI agents investigated him for possible communist ties, recorded his extramarital affairs and reported on them to government officials, and, in 1964, mailed King a threatening anonymous letter, which he interpreted as an attempt to make him commit suicide.[3]\n",
      "\n",
      "On October 14, 1964, King won the Nobel Peace Prize for combating racial inequality through nonviolent resistance. In 1965, he helped organize two of the three Selma to Montgomery marches. In his final years, he expanded his focus to include opposition towards poverty, capitalism, and the Vietnam War.\n",
      "\n",
      "In 1968, King was planning a national occupation of Washington, D.C., to be called the Poor People's Campaign, when he was assassinated on April 4 in Memphis, Tennessee. His death was followed by riots in many U.S. cities. Allegations that James Earl Ray, the man convicted of killing King, had been framed or acted in concert with government agents persisted for decades after the shooting. King was posthumously awarded the Presidential Medal of Freedom in 1977 and the Congressional Gold Medal in 2003. Martin Luther King Jr. Day was established as a holiday in cities and states throughout the United States beginning in 1971; the holiday was enacted at the federal level by legislation signed by President Ronald Reagan in 1986. Hundreds of streets in the U.S. have been renamed in his honor, and the most populous county in Washington State was rededicated for him. The Martin Luther King Jr. Memorial on the National Mall in Washington, D.C., was dedicated in 2011.\n",
      "(451313080118390996, 0, 1) Martin\n",
      "(451313080118390996, 1, 2) Luther\n",
      "(451313080118390996, 2, 3) King\n",
      "(451313080118390996, 3, 4) Jr.\n",
      "(451313080118390996, 6, 7) Michael\n",
      "(451313080118390996, 7, 8) King\n",
      "(451313080118390996, 8, 9) Jr.\n",
      "(451313080118390996, 10, 11) January\n",
      "(451313080118390996, 15, 16) April\n",
      "(451313080118390996, 23, 24) Baptist\n",
      "(451313080118390996, 49, 50) King\n",
      "(451313080118390996, 69, 70) Mahatma\n",
      "(451313080118390996, 70, 71) Gandhi\n",
      "(451313080118390996, 83, 84) Martin\n",
      "(451313080118390996, 84, 85) Luther\n",
      "(451313080118390996, 85, 86) King\n",
      "(451313080118390996, 86, 87) Sr\n",
      "(451313080118390996, 87, 88) .\n",
      "(451313080118390996, 89, 90) King\n",
      "(451313080118390996, 113, 114) King\n",
      "(451313080118390996, 117, 118) Montgomery\n",
      "(451313080118390996, 128, 129) Southern\n",
      "(451313080118390996, 129, 130) Christian\n",
      "(451313080118390996, 130, 131) Leadership\n",
      "(451313080118390996, 131, 132) Conference\n",
      "(451313080118390996, 133, 134) SCLC\n",
      "(451313080118390996, 140, 141) SCLC\n",
      "(451313080118390996, 146, 147) Albany\n",
      "(451313080118390996, 147, 148) Movement\n",
      "(451313080118390996, 149, 150) Albany\n",
      "(451313080118390996, 151, 152) Georgia\n",
      "(451313080118390996, 163, 164) Birmingham\n",
      "(451313080118390996, 165, 166) Alabama\n",
      "(451313080118390996, 167, 168) King\n",
      "(451313080118390996, 172, 173) March\n",
      "(451313080118390996, 174, 175) Washington\n",
      "(451313080118390996, 193, 194) Lincoln\n",
      "(451313080118390996, 194, 195) Memorial\n",
      "(451313080118390996, 198, 199) SCLC\n",
      "(451313080118390996, 240, 241) Federal\n",
      "(451313080118390996, 241, 242) Bureau\n",
      "(451313080118390996, 243, 244) Investigation\n",
      "(451313080118390996, 245, 246) FBI\n",
      "(451313080118390996, 247, 248) Director\n",
      "(451313080118390996, 248, 249) J.\n",
      "(451313080118390996, 249, 250) Edgar\n",
      "(451313080118390996, 250, 251) Hoover\n",
      "(451313080118390996, 252, 253) King\n",
      "(451313080118390996, 262, 263) FBI\n",
      "(451313080118390996, 264, 265) COINTELPRO\n",
      "(451313080118390996, 270, 271) FBI\n",
      "(451313080118390996, 297, 298) King\n",
      "(451313080118390996, 317, 318) October\n",
      "(451313080118390996, 322, 323) King\n",
      "(451313080118390996, 325, 326) Nobel\n",
      "(451313080118390996, 326, 327) Peace\n",
      "(451313080118390996, 327, 328) Prize\n",
      "(451313080118390996, 346, 347) Selma\n",
      "(451313080118390996, 348, 349) Montgomery\n",
      "(451313080118390996, 370, 371) Vietnam\n",
      "(451313080118390996, 371, 372) War\n",
      "(451313080118390996, 377, 378) King\n",
      "(451313080118390996, 384, 385) Washington\n",
      "(451313080118390996, 386, 387) D.C.\n",
      "(451313080118390996, 392, 393) Poor\n",
      "(451313080118390996, 393, 394) People\n",
      "(451313080118390996, 402, 403) April\n",
      "(451313080118390996, 405, 406) Memphis\n",
      "(451313080118390996, 407, 408) Tennessee\n",
      "(451313080118390996, 417, 418) U.S.\n",
      "(451313080118390996, 422, 423) James\n",
      "(451313080118390996, 423, 424) Earl\n",
      "(451313080118390996, 424, 425) Ray\n",
      "(451313080118390996, 431, 432) King\n",
      "(451313080118390996, 450, 451) King\n",
      "(451313080118390996, 455, 456) Presidential\n",
      "(451313080118390996, 456, 457) Medal\n",
      "(451313080118390996, 458, 459) Freedom\n",
      "(451313080118390996, 463, 464) Congressional\n",
      "(451313080118390996, 464, 465) Gold\n",
      "(451313080118390996, 465, 466) Medal\n",
      "(451313080118390996, 469, 470) Martin\n",
      "(451313080118390996, 470, 471) Luther\n",
      "(451313080118390996, 471, 472) King\n",
      "(451313080118390996, 472, 473) Jr.\n",
      "(451313080118390996, 473, 474) Day\n",
      "(451313080118390996, 485, 486) United\n",
      "(451313080118390996, 486, 487) States\n",
      "(451313080118390996, 503, 504) President\n",
      "(451313080118390996, 504, 505) Ronald\n",
      "(451313080118390996, 505, 506) Reagan\n",
      "(451313080118390996, 514, 515) U.S.\n",
      "(451313080118390996, 528, 529) Washington\n",
      "(451313080118390996, 529, 530) State\n",
      "(451313080118390996, 536, 537) Martin\n",
      "(451313080118390996, 537, 538) Luther\n",
      "(451313080118390996, 538, 539) King\n",
      "(451313080118390996, 539, 540) Jr.\n",
      "(451313080118390996, 540, 541) Memorial\n",
      "(451313080118390996, 543, 544) National\n",
      "(451313080118390996, 544, 545) Mall\n",
      "(451313080118390996, 546, 547) Washington\n",
      "(451313080118390996, 548, 549) D.C.\n"
     ]
    }
   ],
   "source": [
    "#We can make complex matches like selecting all the instances of a verb followed by noun.\n",
    "#This is a more powerful feature of spacy which are not present in other inferior libraries.\n",
    "with open(\"wiki_mlk.txt\",\"r\") as f:\n",
    "    text=f.read()\n",
    "\n",
    "print(text)\n",
    "#Lets extract all the proper nouns in this text.\n",
    "\n",
    "nlp3=spacy.load(\"en_core_web_md\")\n",
    "matcher=Matcher(nlp3.vocab)\n",
    "pattern=[{\"POS\":\"PROPN\"}] #This extracts any occurance of part of speech where the word is a proper noun.\n",
    "matcher.add(\"PROPER_NOUN\",[pattern])\n",
    "doc3=nlp3(text)\n",
    "matches=matcher(doc3)\n",
    "\n",
    "for match in matches:\n",
    "    print(match,doc3[match[1]:match[2]])\n",
    "#This can extract all the single word proper nouns but it is limited\n",
    "#As if the whole name of a person should be extract  as single but here it takes each individually\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(451313080118390996, 0, 4) Martin Luther King Jr.\n",
      "(451313080118390996, 6, 9) Michael King Jr.\n",
      "(451313080118390996, 10, 11) January\n",
      "(451313080118390996, 15, 16) April\n",
      "(451313080118390996, 23, 24) Baptist\n",
      "(451313080118390996, 49, 50) King\n",
      "(451313080118390996, 69, 71) Mahatma Gandhi\n",
      "(451313080118390996, 83, 88) Martin Luther King Sr.\n",
      "(451313080118390996, 89, 90) King\n",
      "(451313080118390996, 113, 114) King\n",
      "(451313080118390996, 117, 118) Montgomery\n",
      "(451313080118390996, 128, 132) Southern Christian Leadership Conference\n",
      "(451313080118390996, 133, 134) SCLC\n",
      "(451313080118390996, 140, 141) SCLC\n",
      "(451313080118390996, 146, 148) Albany Movement\n",
      "(451313080118390996, 149, 150) Albany\n",
      "(451313080118390996, 151, 152) Georgia\n",
      "(451313080118390996, 163, 164) Birmingham\n",
      "(451313080118390996, 165, 166) Alabama\n",
      "(451313080118390996, 167, 168) King\n",
      "(451313080118390996, 172, 173) March\n",
      "(451313080118390996, 174, 175) Washington\n",
      "(451313080118390996, 193, 195) Lincoln Memorial\n",
      "(451313080118390996, 198, 199) SCLC\n",
      "(451313080118390996, 240, 242) Federal Bureau\n",
      "(451313080118390996, 243, 244) Investigation\n",
      "(451313080118390996, 245, 246) FBI\n",
      "(451313080118390996, 247, 251) Director J. Edgar Hoover\n",
      "(451313080118390996, 252, 253) King\n",
      "(451313080118390996, 262, 263) FBI\n",
      "(451313080118390996, 264, 265) COINTELPRO\n",
      "(451313080118390996, 270, 271) FBI\n",
      "(451313080118390996, 297, 298) King\n",
      "(451313080118390996, 317, 318) October\n",
      "(451313080118390996, 322, 323) King\n",
      "(451313080118390996, 325, 328) Nobel Peace Prize\n",
      "(451313080118390996, 346, 347) Selma\n",
      "(451313080118390996, 348, 349) Montgomery\n",
      "(451313080118390996, 370, 372) Vietnam War\n",
      "(451313080118390996, 377, 378) King\n",
      "(451313080118390996, 384, 385) Washington\n",
      "(451313080118390996, 386, 387) D.C.\n",
      "(451313080118390996, 392, 394) Poor People\n",
      "(451313080118390996, 402, 403) April\n",
      "(451313080118390996, 405, 406) Memphis\n",
      "(451313080118390996, 407, 408) Tennessee\n",
      "(451313080118390996, 417, 418) U.S.\n",
      "(451313080118390996, 422, 425) James Earl Ray\n",
      "(451313080118390996, 431, 432) King\n",
      "(451313080118390996, 450, 451) King\n",
      "(451313080118390996, 455, 457) Presidential Medal\n",
      "(451313080118390996, 458, 459) Freedom\n",
      "(451313080118390996, 463, 466) Congressional Gold Medal\n",
      "(451313080118390996, 469, 474) Martin Luther King Jr. Day\n",
      "(451313080118390996, 485, 487) United States\n",
      "(451313080118390996, 503, 506) President Ronald Reagan\n",
      "(451313080118390996, 514, 515) U.S.\n",
      "(451313080118390996, 528, 530) Washington State\n",
      "(451313080118390996, 536, 541) Martin Luther King Jr. Memorial\n",
      "(451313080118390996, 543, 545) National Mall\n",
      "(451313080118390996, 546, 547) Washington\n",
      "(451313080118390996, 548, 549) D.C.\n"
     ]
    }
   ],
   "source": [
    "#To rectify the error of the previous notebook, I have to change the code to the following\n",
    "nlp3=spacy.load(\"en_core_web_md\")\n",
    "matcher=Matcher(nlp3.vocab)\n",
    "pattern=[{\"POS\":\"PROPN\",\"OP\":\"+\"}]#This op detrmines how often to make the match to token pattern\n",
    "#Here the '+' means to match with token one or more times\n",
    "#What this means is it can match the single token as well as with multiple tokens which are compounded\n",
    "\n",
    "matcher.add(\"PROPER_NOUN\",[pattern],greedy='LONGEST') \n",
    "#THis takes matches single as well as compound and in case of compound it will take the longest one\n",
    "\n",
    "doc3=nlp3(text)\n",
    "matches=matcher(doc3)\n",
    "\n",
    "matches.sort(key=lambda x:x[1]) #This is to sort as the matches according to beginning occurance of the longest token\n",
    "\n",
    "for match in matches:\n",
    "    print(match,doc3[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(451313080118390996, 49, 51) King advanced\n",
      "(451313080118390996, 89, 91) King participated\n",
      "(451313080118390996, 113, 115) King led\n",
      "(451313080118390996, 198, 200) SCLC put\n",
      "(451313080118390996, 247, 252) Director J. Edgar Hoover considered\n",
      "(451313080118390996, 322, 324) King won\n",
      "(451313080118390996, 485, 488) United States beginning\n"
     ]
    }
   ],
   "source": [
    "#Now lets make it more complicated lets find matches where a proper noun is followed by a verb\n",
    "nlp3=spacy.load(\"en_core_web_md\")\n",
    "matcher=Matcher(nlp3.vocab)\n",
    "pattern=[{\"POS\":\"PROPN\",\"OP\":\"+\"},{\"POS\":\"VERB\"}]\n",
    "matcher.add(\"PROPER_NOUN\",[pattern],greedy='LONGEST') \n",
    "\n",
    "doc3=nlp3(text)\n",
    "matches=matcher(doc3)\n",
    "\n",
    "matches.sort(key=lambda x:x[1]) #This is to sort as the matches according to beginning occurance of the longest token\n",
    "\n",
    "for match in matches:\n",
    "    print(match,doc3[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, `and what is the use of a book,' thought Alice `without pictures or conversation?'\n",
      "Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, `and what is the use of a book,' thought Alice `without pictures or conversation?'\n",
      "1\n",
      "(3232560085755078826, 47, 60) `and what is the use of a book,' thought Alice\n"
     ]
    }
   ],
   "source": [
    "#Now lets do a more complex tash, we will take a paragraph and find all the instances where a certain quote is given by some one\n",
    "import json\n",
    "nlp4=spacy.load(\"en_core_web_md\")\n",
    "matcher4=Matcher(nlp3.vocab)\n",
    "with open (\"alice.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "text = data[0][2][0]\n",
    "print (text)\n",
    "\n",
    "text.replace(\"`\", \"'\") #This is somehow not working\n",
    "print (text)\n",
    "\n",
    "speak_lemmas = [\"think\", \"say\"]\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{'ORTH': \"`\"}, #So the sequence must start with '\n",
    "           {'IS_ALPHA': True, \"OP\": \"+\"},#Then followed by one or more alphabetic tokens\n",
    "            {'IS_PUNCT': True, \"OP\": \"*\"}, #Could possibly be a punctuation mark like comma\n",
    "            {'ORTH': \"'\"},#Terminated by '\n",
    "            {'POS':'VERB',\"LEMMA\":{\"IN\":speak_lemmas}}, #This checks if the quote is followed by a certain version of verb Speak\n",
    "            {'POS':'PROPN', 'OP': '+'},\n",
    "            ]\n",
    "#We can also add more than one pattern as there can be multiple slightly different variant of the equired pattern.\n",
    "matcher.add(\"PROPER_NOUNS\", [pattern], greedy='LONGEST')\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "matches.sort(key = lambda x: x[1])\n",
    "print (len(matches))\n",
    "for match in matches[:10]:\n",
    "    print (match, doc[match[1]:match[2]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
