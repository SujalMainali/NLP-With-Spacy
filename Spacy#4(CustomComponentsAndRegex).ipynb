{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Britain\n",
      "Britain GPE\n",
      "Mary\n",
      "Mary PERSON\n"
     ]
    }
   ],
   "source": [
    "#A custom component is something that we need to do to the doc object that we cant do with normal spacy\n",
    "\n",
    "#Lets create a custom pipe that will never give an object entity as GPE always LOC or nothing\n",
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "doc=nlp(\"Britain is a place. Mary is a person\")\n",
    "for ent in doc.ents:\n",
    "    print(ent)\n",
    "    print(ent.text,ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': {'tok2vec': {'assigns': ['doc.tensor'],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'tagger': {'assigns': ['token.tag'],\n",
       "   'requires': [],\n",
       "   'scores': ['tag_acc'],\n",
       "   'retokenizes': False},\n",
       "  'parser': {'assigns': ['token.dep',\n",
       "    'token.head',\n",
       "    'token.is_sent_start',\n",
       "    'doc.sents'],\n",
       "   'requires': [],\n",
       "   'scores': ['dep_uas',\n",
       "    'dep_las',\n",
       "    'dep_las_per_type',\n",
       "    'sents_p',\n",
       "    'sents_r',\n",
       "    'sents_f'],\n",
       "   'retokenizes': False},\n",
       "  'attribute_ruler': {'assigns': [],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'lemmatizer': {'assigns': ['token.lemma'],\n",
       "   'requires': [],\n",
       "   'scores': ['lemma_acc'],\n",
       "   'retokenizes': False},\n",
       "  'ner': {'assigns': ['doc.ents', 'token.ent_iob', 'token.ent_type'],\n",
       "   'requires': [],\n",
       "   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n",
       "   'retokenizes': False}},\n",
       " 'problems': {'tok2vec': [],\n",
       "  'tagger': [],\n",
       "  'parser': [],\n",
       "  'attribute_ruler': [],\n",
       "  'lemmatizer': [],\n",
       "  'ner': []},\n",
       " 'attrs': {'token.tag': {'assigns': ['tagger'], 'requires': []},\n",
       "  'doc.ents': {'assigns': ['ner'], 'requires': []},\n",
       "  'doc.sents': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.lemma': {'assigns': ['lemmatizer'], 'requires': []},\n",
       "  'token.dep': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.ent_type': {'assigns': ['ner'], 'requires': []},\n",
       "  'doc.tensor': {'assigns': ['tok2vec'], 'requires': []},\n",
       "  'token.is_sent_start': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.ent_iob': {'assigns': ['ner'], 'requires': []},\n",
       "  'token.head': {'assigns': ['parser'], 'requires': []}}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To fulfill the goal lets first import languages\n",
    "from spacy.language import Language\n",
    "@Language.component(\"remove_gpe_custom\")\n",
    "def remove_gpe_custom(doc):\n",
    "    original_ents=list(doc.ents)\n",
    "    for ent in original_ents:\n",
    "        if ent.label_==\"GPE\":\n",
    "            original_ents.remove(ent)\n",
    "    doc.ents=original_ents\n",
    "    print(doc.ents)\n",
    "    return doc\n",
    "\n",
    "#Here we defined a function remove_gpe and decleared it as a custom component. Then, we will be able to manipuate the metadata itself\n",
    "\n",
    "#Lets add the remove_gpe pipe to the nlp object at the end of the pipeline\n",
    "\n",
    "#nlp.add_pipe(\"remove_gpe_custom\") #This can only be added once since we can only add same named pipe once.\n",
    "\n",
    "nlp.analyze_pipes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Britain GPE\n",
      "Britain\n",
      "Mary PERSON\n",
      "Mary\n"
     ]
    }
   ],
   "source": [
    "#Now we reanalize the text and see the output with the custom pipe.\n",
    "doc=nlp(\"Britain is a place. Mary is a person\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text,ent.label_)\n",
    "    print(ent)\n",
    "\n",
    "#If we made some useful modifications to the pipe. Then, we can save it for future use by saving it to disk.\n",
    "nlp.to_disk(\"custom_en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('2 February', '2', 'February'), ('14 April', '4', 'April')]\n",
      "[('2 February', '2 February', '2', 'February', '', '', ''), ('14 April', '14 April', '4', 'April', '', '', '')]\n",
      "[('2 February', '2 February', '2', 'February', '', '', ''), ('April 14', '', '', '', 'April 14', 'April', '4')]\n",
      "2 February\n",
      "April 14\n"
     ]
    }
   ],
   "source": [
    "#RegEx is a way of achieving complex string matching based on simple or complex patterns.\n",
    "#We can apply complex rule to find or match text in very short space.\n",
    "#RegEx comes prepackages with python. We can start using it by importing it.\n",
    "import re\n",
    "\n",
    "pattern= r\"((\\d){1,2} (January|February|March|April))\"#This checks for 1 or 2 digits followed by any of these words.\n",
    "text = \"This is a date 2 February. Another date would be 14 April.\"\n",
    "matches = re.findall(pattern, text)\n",
    "print (matches)\n",
    "\n",
    "#If we want to add more complex regex we can add alternative pattern rules\n",
    "pattern= r\"(((\\d){1,2} (January|February|March|April))|((January|February|March|April) (\\d){1,2}))\"\n",
    "text = \"This is a date 2 February. Another date would be 14 April.\"\n",
    "matches = re.findall(pattern, text)\n",
    "print (matches)\n",
    "text = \"This is a date 2 February. Another date would be April 14.\"\n",
    "matches = re.findall(pattern, text)\n",
    "print (matches)\n",
    "\n",
    "for match in matches:\n",
    "    print(match[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<callable_iterator object at 0x7fab6e208070>\n",
      "<re.Match object; span=(15, 25), match='February 2'>\n",
      "February 2\n",
      "<re.Match object; span=(49, 57), match='14 April'>\n",
      "14 April\n"
     ]
    }
   ],
   "source": [
    "#We can use finditer to get more useful information when searching for patterns in text.\n",
    "text = \"This is a date February 2. Another date would be 14 April.\"\n",
    "iter_matches = re.finditer(pattern, text)\n",
    "print (iter_matches)\n",
    "for hit in iter_matches:\n",
    "    print (hit)\n",
    "    start = hit.start()\n",
    "    end = hit.end()\n",
    "    print (text[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets learn to use RegEx in Spacy to implement with EntityRuler.\n",
    "#Lets use RegEx to Match numbers like 555-5555 to PhoneNumber Label.\n",
    "\n",
    "text= \"This is a sampple number 333 555-5555\"\n",
    "\n",
    "nlp=spacy.blank(\"en\")\n",
    "\n",
    "ruler=nlp.add_pipe(\"entity_ruler\")\n",
    "\n",
    "patterns=[{\n",
    "    \"label\":\"PhoneNumber\",\n",
    "    \"pattern\":[{\"TEXT\":{\"REGEX\":\"((\\d){3}-(\\d){4})\"}}]\n",
    "}]\n",
    "\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "doc=nlp(text)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text,ent.label_)  \n",
    "\n",
    "#This didnt work because the '-' in the phone number confuses the entity ruler. If there was no dash it would have worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 11), match='Paul Newman'>\n",
      "<re.Match object; span=(39, 53), match='Paul Hollywood'>\n"
     ]
    }
   ],
   "source": [
    "#We want to use RegEx only when the patterns we want to match is independent of the linguistic features that Spacy offers.\n",
    "\n",
    "#Also, when we use matcher to match multi-word patterns called sppan they are not automatically added to the entities. Here we will use RegEx tp extract multiword tokens and add them to entities.\n",
    "text = \"Paul Newman was an American actor, but Paul Hollywood is a British TV Host. The name Paul is quite common.\"\n",
    "\n",
    "pattern = r\"Paul [A-Z]\\w+\" #This searches for words which conntain Paul followed by capital letter followed by tthe word break\n",
    "\n",
    "matches = re.finditer(pattern, text)\n",
    "\n",
    "for match in matches:\n",
    "    print (match)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paul Newman\n",
      "Paul Hollywood\n",
      "[(0, 2, 'Paul Newman'), (8, 10, 'Paul Hollywood')]\n",
      "Paul Newman PERSON\n",
      "Paul Hollywood PERSON\n"
     ]
    }
   ],
   "source": [
    "#Now lets \n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp=spacy.blank(\"en\")\n",
    "doc= nlp(text)\n",
    "original_ents=list(doc.ents)\n",
    "mwt_ents=[] #Multiword roken entities\n",
    "pattern = r\"Paul [A-Z]\\w+\"\n",
    "for match in re.finditer(pattern,text):\n",
    "    start,end=match.span() #This records the start and end location of the matched word.\n",
    "    #This start and end consists of location of characters not tokens. So, we extract spans from this by,\n",
    "    span=doc.char_span(start,end) #We are adding span using character pointers but when they are stored in span they are stored as token pointers\n",
    "    print(span)\n",
    "    if span is not None:\n",
    "        mwt_ents.append((span.start,span.end,span.text))\n",
    "\n",
    "print(mwt_ents) #This finally obtains the matched spans in a format used by Spacy.\n",
    "\n",
    "for ent in mwt_ents:\n",
    "    start,end,text=ent\n",
    "    per_ent=Span(doc,start,end,label=\"PERSON\") #This starts the MultiWord Span as a Entity of Label PERSON\n",
    "    original_ents.append(per_ent)\n",
    "doc.ents=original_ents\n",
    "for ent in doc.ents:\n",
    "    print(ent.text,ent.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paul Newman\n",
      "Paul Hollywood\n",
      "(Paul Newman, Paul Hollywood)\n"
     ]
    }
   ],
   "source": [
    "#For simplicity lets store this code to Spacy Object as a custom component.\n",
    "@Language.component(\"MultiWordToken\")\n",
    "def MultiWordToken(doc):\n",
    "    original_ents=list(doc.ents)\n",
    "    mwt_ents=[] #Multiword roken entities\n",
    "    for match in re.finditer(pattern,doc.text):\n",
    "        start,end=match.span() #This records the start and end location of the matched word.\n",
    "        #This start and end consists of location of characters not tokens. So, we extract spans from this by,\n",
    "        span=doc.char_span(start,end)\n",
    "        print(span)\n",
    "        if span is not None:\n",
    "            mwt_ents.append((span.start,span.end,span.text))\n",
    "\n",
    "    for ent in mwt_ents:\n",
    "        start,end,text=ent\n",
    "        per_ent=Span(doc,start,end,label=\"PERSON\") #This starts the MultiWord Span as a Entity of Label PERSON\n",
    "        original_ents.append(per_ent)\n",
    "    doc.ents=original_ents\n",
    "    return doc\n",
    "\n",
    "nlp2=spacy.blank(\"en\")\n",
    "nlp2.add_pipe(\"MultiWordToken\")\n",
    "text = \"Paul Newman was an American actor, but Paul Hollywood is a British TV Host. The name Paul is quite common.\"\n",
    "doc2=nlp2(text)\n",
    "print(doc2.ents)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
